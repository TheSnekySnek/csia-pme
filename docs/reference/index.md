# Reference

## Team

### Andrea Petrucci

- Role: Developer
- Site: Fribourg
- Email: [andrea.petrucci@hefr.ch](andrea.petrucci@hefr.ch)
- Works on:
    - Monday
    - Tuesday
    - Thursday morning

### Ludovic Delafontaine

- Role: Developer
- Specialization(s): MLOps
- Site: HEIG-VD (Yverdon-les-Bains)
- Email: [ludovic.delafontaine@heig-vd.ch](mailto:ludovic.delafontaine@heig-vd.ch)
- Works on:
    - Monday
    - Tuesday
    - Wednesday


## Docker base image

## Overview

## Engine

# Engine

- [Code](../../engine)
- Engine URL when run locally: <http://localhost:8080/docs>
- Engine URL when deployed on Fribourg's Kubernetes: <https://engine-csia-pme.kube.isc.heia-fr.ch/docs>
- MinIO Console URL when deployed on Fribourg's Kubernetes: <https://console-minio-csia-pme.kube.isc.heia-fr.ch>

## Description

This service allows to create and manage pipelines of microservices. This service was built and tested with python 3.9, therefore we recommend to use the docker version instead of running it natively.

The API documentation is automatically generated by FastAPI using the OpenAPI standard. A user friendly interface provided by Swagger is available under the `/docs` route, where the endpoints of the services are described.

## Pipelines

The `pipelines` folder contains several examples of ready to use pipelines:

- `blur-women.json`: Blur women detected in an image.
- `convertjpg.json`: Convert any image to jpg.
- `face-thumbs.json`: Generate a thumbnail of each face detected in the image.
- `faceblur.json`: Blur every face detected in an image.
- `people-description.json`: Analyzes every face in an image and returns a global description.

## How to run

### Environment variables

The engine will use the following environment variables if defined.

*General variables*

- `APP_HOST`: address on which the API will listen, default is 127.0.0.1
- `APP_PORT`: port the API will listen on, default is 8080
- `APP_LOG`: log level, default is info
- `APP_ENGINE`: the url to itself, needed to give the correct callback url to services
- `APP_EXTERNAL_URL`: the external url which the engine is accessed to, needed to build the correct url for the results
- `APP_CRON`: the frequency in seconds of the job that will check and clean old jobs and data, default if 300. Each 20 times this value, a job will clean all unreferenced objects on the storage that have a very old timestamp (10 times `APP_LIFESPAN`).
- `APP_LIFESPAN`: the lifespan in seconds of jobs and data, passed this delay, they are cleaned, default is 1800
- `APP_RETRY_CRON`: the frequency in seconds of the retry task, default is 30
- `APP_CLEAN_SERVICE_CRON`: the frequency in seconds of the job that will discard services if they don't announce themself frequently enough

*Storage specific variables*

- `REG_STORAGE_TYPE`: set which type of object storage to use, possible values are "local" or "S3", default is local
- `REG_LOCAL_DATA`: path to the folder where the objects should be stored, only applicable for a "local" storage type
- `S3_URL`: url to the S3 storage server, only applicable for a "S3" storage type
- `S3_ZONE`: zone of the S3 storage server, only applicable for a "S3" storage type
- `S3_KEY_ID`: key to connect to the S3 storage server, only applicable for a "S3" storage type
- `S3_SECRET_KEY`: secret to connect to the S3 storage server, only applicable for a "S3" storage type
- `S3_BUCKET`: name of the bucket to use on the S3 storage server, only applicable for a "S3" storage type

*Database specific variables*

- `REG_DB_TYPE`: define which type of storage to use for the data persistency, possible values are "memory" or "mongo", default is memory
- `MONGO_DB`: name of the mongo database to use, only applicable if database type is mongo
- `MONGO_URI`: uri to use to connect to the mongo server, usually `mongodb://MONGOUSER:MONGOPASSWD@HOST:PORT`, only applicable if database type is mongo

### Start the application

In the [engine](../../engine) directory, start the Engine with the following commands.

Generate the virtual environment and install the dependencies.

```sh
# Generate the virtual environment
python3 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate

# Install the requirements
pip install --requirement requirements.txt
```

Start the application.

```sh
# Start the application
APP_HOST=0.0.0.0 APP_PORT=8080 APP_ENGINE=http://localhost:8080 python3 main.py
```

Access the Engine documentation on <http://localhost:8080/docs>.

### Run the tests

In the [engine](../../engine) directory, run the tests with the following commands.

Install the additional packages.

```sh
# Install required packages for testing
pip3 install pytest pytest-asyncio aiofile
```

Run the tests.

```sh
# Run the tests
python3 -m pytest --asyncio-mode=auto
```

### Run locally using Kubernetes (with minikube) and official Docker images

Start the Engine with the following commands. This will start the Engine with the official Docker images that are hosted on GitHub.

In the [engine](../../engine) directory, start the Engine with the following commands.

```sh
# Start MinIO
kubectl apply \
    -f kubernetes/minio.pvc.yml \
    -f kubernetes/minio.config-map.yml \
    -f kubernetes/minio.stateful.yml \
    -f kubernetes/minio.service.yml

# Start Mongo
kubectl apply \
    -f kubernetes/mongo.pvc.yml \
    -f kubernetes/mongo.config-map.yml \
    -f kubernetes/mongo.stateful.yml \
    -f kubernetes/mongo.service.yml

# Start the engine
kubectl apply \
    -f kubernetes/engine.config-map.yml \
    -f kubernetes/engine.stateful.yml \
    -f kubernetes/engine.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

### Run locally using Kubernetes (with minikube) and a local Docker image

**Note**: The Engine StatefulSet (`engine.stateful.yml` file) must be deleted and recreated every time a new Docker image is created.

Start the Engine with the following commands. This will start the Engine with the a local Docker image for the Engine.

In the [engine](../../engine) directory, build the Docker image with the following commands.

```sh
# Access the Minikube's Docker environment
eval $(minikube docker-env)

# Build the Docker image
docker build -t ghcr.io/csia-pme/csia-pme-engine:latest .

# Exit the Minikube's Docker environment
eval $(minikube docker-env -u)

# Edit the `kubernetes/engine.stateful.yml` file to use the local image by uncommented the line `imagePullPolicy`
#
# From
#
#        # imagePullPolicy: Never
#
# To
#
#        imagePullPolicy: Never
```

In the [engine](../../engine) directory, start the Engine with the following commands.

```sh
# Start MinIO
kubectl apply \
    -f kubernetes/minio.pvc.yml \
    -f kubernetes/minio.config-map.yml \
    -f kubernetes/minio.stateful.yml \
    -f kubernetes/minio.service.yml

# Start Mongo
kubectl apply \
    -f kubernetes/mongo.pvc.yml \
    -f kubernetes/mongo.config-map.yml \
    -f kubernetes/mongo.stateful.yml \
    -f kubernetes/mongo.service.yml

# Start the engine
kubectl apply \
    -f kubernetes/engine.config-map.yml \
    -f kubernetes/engine.stateful.yml \
    -f kubernetes/engine.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the Engine documentation on <http://localhost:8080/docs>.


NEW README

# Engine

- [Code](../../engine)
- Engine URL when run locally: <http://localhost:8080/docs>
- Engine URL when deployed on Fribourg's Kubernetes: <https://engine-csia-pme.kube.isc.heia-fr.ch/docs>
- MinIO Console URL when deployed on Fribourg's Kubernetes: <https://console-minio-csia-pme.kube.isc.heia-fr.ch>

## Description

This service allows to create and manage pipelines of microservices. This service was built and tested with python 3.9, therefore we recommend to use the docker version instead of running it natively.

### Running

You can start the Engine with the following commands.

```bash
# Start the infrastructure from the `engine` directory
docker compose up

# Start the app from the `src` directory
uvicorn --reload main:app
```

### Testing

For each module a test file is available to check the correct behavior of the code. The tests are run using the `pytest` library with code coverage check. To run the tests, use the following command inside the `engine` folder:

```bash
pytest --cov-report term:skip-covered --cov-report term-missing --cov=. -s --cov-config=.coveragerc
```

### SQLModel

This part of the code has been built by following the [SQLModel tutorial](https://sqlmodel.tiangolo.com/tutorial/) and using DDD (Domain Driven Design) to keep a clean and simple code.

The models defined for each classes is represented as follows:

![UML Diagram](./models_diagram.svg)

A normal sequence of events when launching a request to a service is represented as follows:

![Sequence Diagram](./sequence_diagram.svg)

The API documentation is automatically generated by FastAPI using the OpenAPI standard. A user friendly interface provided by Swagger is available under the `/docs` route, where the endpoints of the services are described.

## Pipelines

The `pipelines` folder contains several examples of ready to use pipelines:

- `blur-women.json`: Blur women detected in an image.
- `convertjpg.json`: Convert any image to jpg.
- `face-thumbs.json`: Generate a thumbnail of each face detected in the image.
- `faceblur.json`: Blur every face detected in an image.
- `people-description.json`: Analyzes every face in an image and returns a global description.

```mermaid
sequenceDiagram
    participant E as e - Engine 
    participant S as s - Service
    participant C as c - Client
    participant S3 as s3 - Storage
    S->>+E: POST(engine_url: str, service_json: ServiceCreate)
    E->>E: service: Service = Service.from_orm(service_json)
    E->>E: enable_service(service)
    E->>E: add_api_route(service.slug, handler)
    E-->>-S: return(200, service: ServiceRead)
    C->>+E: POST(/s.slug, data: UploadFile[])
    E->>+S3: file_keys = for file in data: storage_service.upload(file)
    S3-->>-E: return(200, file_key)
    E->>E: task = create_task(data_in: file_keys[])
    E->>E: service_task = new ServiceTask(s3_infos: str[], task)
    E->>+S: POST(s.url, service_task)
    S-->>-E: return(200, {status: Task added to the queue})
    E-->>-C: return(200, task: TaskReadWithServiceAndPipeline)
    S->>+S3: data = for key in service_task.task.data_in: get_file(service_task.s3_infos, key)
    S3-->>-S: return(200, stream)
    S->>S: result = process(data)
    S->>+S3: data_out = for res in result: upload_file(service_task.s3_infos, data_out)
    S3-->>-S: return(200, key)
    S->>S: task_update = jsonable_encoder(TaskUpdate({status: finished, task.data_out: data_out}))
    S->>+E: PATCH(engine_url: str, task_update)
    E-->-S: return(200, service: ServiceRead)
    C->>+E: GET(/tasks, task_id: str)
    E-->>-C: return(200, task: TaskReadWithServiceAndPipeline)
    C->>+S3: GET(task.data_out)
    S3-->>-C: return(200, stream)
```

```mermaid
sequenceDiagram
    participant S as s - Service
    participant C as c - Client
    participant S3 as s3 - Storage
    C->>+S3: file_keys = for file in data: upload(file)
    S3-->>-C: return(200, file_key)
    C->>+S: POST(s.url, callback_url: str, service_task: ServiceTask)
    Note right of S: callback_url is the url where the service should send the response
    Note right of S: service_task should match the model
    S-->>-C: return(200, Task added to the queue)
    S->>+S3: data = for key in service_task.task.data_in: get_file(service_task.s3_infos, key)
    S3-->>-S: return(200, stream)
    S->>S: result = process(data)
    S->>+S3: data_out = for res in result: upload_file(service_task.s3_infos, data_out)
    S3-->>-S: return(200, key)
    S->>S: task_update = jsonable_encoder(TaskUpdate({status: finished, task.data_out: data_out}))
    S->>+C: PATCH(callback_url, task_update)
    C-->>-S: return(200, OK)
    C->>+S3: GET(task_update.data_out)
    S3-->>-C: return(200, stream)
```





## Webapp


- [Code](../../webapp)
- Webapp URL when run locally: <http://localhost:8686>
- Webapp URL when deployed on Fribourg's Kubernetes: <https://webapp-csia-pme.kube.isc.heia-fr.ch/docs>

## Description

This service allows to visualize services and pipelines from the [Engine](../engine/readme.md). This service was built and tested with React.

## How to run

### Environment variables

The service will use the following environment variables if defined.

*General variables*

- `REACT_APP_ENGINE_URL`: The Engine URL

**Note**: The environment variables are replaced while building the React application.

## Run the application

In the [webapp](../../webapp) directory, start the Webapp with the following commands.

```sh
# Install the dependencies
npm ci --legacy-peer-deps

# Optional: Edit the environment variables to change the Engine URL
vim .env

# Start the Webapp
npm run start
```

A browser should open on <http://localhost:3000> with the Webapp running and querying the Engine.

## Build the application

In the [webapp](../../webapp) directory, build the Webapp with the following commands.

```sh
# Install the dependencies
npm ci --legacy-peer-deps

# Build the Webapp
npm run build
```

The output of the build is in the [build](../../webapp/build) directory.

Once a React application is built, the environment variables cannot be changed.

## Build and run the Docker image

In order to build the Docker image, the application must be [built](#build-the-application) beforehand. Then, the Docker image can be built with the following commands.

```sh
# Build the Docker image with a tag
docker build -t csia-pme/webapp .

# Run the Docker image
docker run -p 8686:80 csia-pme/webapp
```

The Webapp is available on <http://localhost:8686>.

> **Q**: _Why don't we build the React application within the Docker image?_\
> **A**: This setup allows us to speed up the build process of the Docker image: it does not need to download and install all dependencies every time the `package.json` file is updated. In a CI/CD set up, the `node_modules` can be cached in the `build` stage and the output can be passed to the `publish` stage.

### Run locally using Kubernetes (with minikube) and official Docker images

Start the Webapp with the following commands. This will start the Webapp with the official Docker images that are hosted on GitHub.

In the [webapp](../../webapp) directory, start the Webapp with the following commands.

```sh
# Start the webapp
kubectl apply \
    -f kubernetes/webapp.config-map.yml \
    -f kubernetes/webapp.stateful.yml \
    -f kubernetes/webapp.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the Webapp on <http://localhost:8686>.

### Run locally using Kubernetes (with minikube) and a local Docker image

**Note**: The service StatefulSet (`webapp.stateful.yml` file) must be deleted and recreated every time a new Docker image is created.

Start the service with the following commands. This will start the service with the a local Docker image for the service.

In the [webapp](../../services/webapp) directory, build the Docker image with the following commands.

```sh
# Install Node dependencies
npm ci --legacy-peer-deps

# Optional: Edit the environment variables to change the Engine URL
vim .env

# Build the Webapp
npm run build

# Access the Minikube's Docker environment
eval $(minikube docker-env)

# Build the Docker image
docker build -t ghcr.io/csia-pme/csia-pme-webapp:latest .

# Exit the Minikube's Docker environment
eval $(minikube docker-env -u)

# Edit the `kubernetes/webapp.stateful.yml` file to use the local image by uncommented the line `imagePullPolicy`
#
# From
#
#        # imagePullPolicy: Never
#
# To
#
#        imagePullPolicy: Never
```

In the [webapp](../../services/webapp) directory, start the service with the following commands.

```sh
# Start the webapp backend
kubectl apply \
    -f kubernetes/webapp.config-map.yml \
    -f kubernetes/webapp.stateful.yml \
    -f kubernetes/webapp.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the Webapp on <http://localhost:8686>.




## Services


### Sample service


- [Code](../../templates/sample_service)
- Access when deployed locally: <http://localhost:8080/docs>

## Description
This is a sample service, to get it running, watch for the TODOs and replace "SAMPLE_SERVICE" and "SAMPLE-SERVICE" in the files with the name of the service.
The core function of the service is the "process" method where the actual processing of the data is done.

TODO: DESCRIBE SERVICE HERE...

TODO: Add your service to [GitHub Actions](../../.github) and once the service is published, change the visibility of the package ([_Configuring visibility of container images for an organization_ - docs.github.com](https://docs.github.com/en/packages/learn-github-packages/configuring-a-packages-access-control-and-visibility#configuring-visibility-of-container-images-for-an-organization)).

## How to run
### Environment variables
Using both docker or your local python3, the engine will use the following environment variables if defined.

*General variables*

- APP_HOST: address on which the API will listen, default is 127.0.0.1
- APP_PORT: port the API will listen on, default is 8080
- APP_LOG: log level, default is info
- APP_ENGINE: the url to the engine, if provided, the service will announce itself to the engine periodically
- APP_SERVICE: the url of the service itself, needed to announce correct routes to the engine
- APP_NOTIFY_CRON: the frequency in second of the heartbeat announce to the engine, default is 30

### Run natively
#### Install dependencies
Install the requirements using `pip3`

```bash
pip3 install -r requirements.txt
```

* In order to work, some packages may need a few additional libs that you can install with your distribution package manager, check console at run time.
#### Run the tests
To run the tests, the following additional packages must be installed:

```bash
pip3 install pytest pytest-asyncio aiofile
```

Then, the tests can be run with:
```bash
python3 -m pytest --asyncio-mode=auto
```

#### Run
Then, you can run the following command to run it in dev:

```bash
python3 main.py
```

or with custom environment variables:

```bash
APP_HOST=0.0.0.0 APP_PORT=4040 APP_LOG=info APP_... python3 main.py
```

### Run locally using Kubernetes

TODO: Update the [Getting started](../guides/getting-started.md) and [Services](../services/readme.md) documentation pages to include the new service

Refer to the [Getting started](../guides/getting-started.md) documentation in order to run this service locally.

### Use
The API documentation is automatically generated by FastAPI using the OpenAPI standard. A user friendly interface provided by Swagger is available under the `/docs` route, where the endpoints of teh service are described.

This simple service only has one route `/compute` that takes an image as input, which will be used to guess the number.




### average_shade


- [Code](../../services/average_shade)
- average_shape URL when run locally: <http://localhost:8282/docs>
- average_shape URL when deployed on Fribourg's Kubernetes: <https://average-shade-csia-pme.kube.isc.heia-fr.ch/docs>

## Description

This service uses numpy and OpenCV to find the average shade color in an image.

The API documentation is automatically generated by FastAPI using the OpenAPI standard. A user friendly interface provided by Swagger is available under the `/docs` route, where the endpoints of teh service are described.

This simple service only has one route `/compute` that takes an image as input, which will be used to guess the number.

## How to run

### Environment variables

The service will use the following environment variables if defined.

*General variables*

- `APP_HOST`: address on which the API will listen, default is 127.0.0.1
- `APP_PORT`: port the API will listen on, default is 8080
- `APP_LOG`: log level, default is info
- `APP_ENGINE`: the url to the engine, if provided, the service will announce itself to the engine periodically
- `APP_SERVICE`: the url of the service itself, needed to announce correct routes to the engine
- `APP_NOTIFY_CRON`: the frequency in second of the heartbeat announce to the engine, default is 30

### Start the application

In the [average_shade](../../services/average_shade) directory, start the service with the following commands.

Generate the virtual environment and install the dependencies.

```sh
# Generate the virtual environment
python3 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate

# Install the requirements
pip install --requirement requirements.txt
```

Start the application.

```sh
# Start the application
APP_HOST=0.0.0.0 APP_PORT=8282 APP_ENGINE=http://localhost:8080 APP_SERVICE=http://localhost:8282 python3 main.py
```

Access the `average_shade` documentation on <http://localhost:8282/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.

### Run the tests

In the [average_shade](../../services/average_shade) directory, run the tests with the following commands.

Install the additional packages.

```sh
# Install required packages for testing
pip3 install pytest pytest-asyncio aiofile
```

Run the tests.

```sh
# Run the tests
python3 -m pytest --asyncio-mode=auto
```

### Run locally using Kubernetes (with minikube) and official Docker images

Start the service with the following commands. This will start the service with the official Docker images that are hosted on GitHub.

In the [average_shade](../../services/average_shade) directory, start the service with the following commands.

```sh
# Start the average_shade backend
kubectl apply \
    -f kubernetes/average-shade.config-map.yml \
    -f kubernetes/average-shade.stateful.yml \
    -f kubernetes/average-shade.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `average_shade` documentation on <http://localhost:8282/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.

### Run locally using Kubernetes (with minikube) and a local Docker image

**Note**: The service StatefulSet (`average-shade.stateful.yml` file) must be deleted and recreated every time a new Docker image is created.

Start the service with the following commands. This will start the service with the a local Docker image for the service.

In the [average_shade](../../services/average_shade) directory, build the Docker image with the following commands.

```sh
# Access the Minikube's Docker environment
eval $(minikube docker-env)

# Build the Docker image
docker build -t ghcr.io/csia-pme/csia-pme-average-shade:latest .

# Exit the Minikube's Docker environment
eval $(minikube docker-env -u)

# Edit the `kubernetes/average-shade.stateful.yml` file to use the local image by uncommented the line `imagePullPolicy`
#
# From
#
#        # imagePullPolicy: Never
#
# To
#
#        imagePullPolicy: Never
```

In the [average_shade](../../services/average_shade) directory, start the service with the following commands.

```sh
# Start the average_shade backend
kubectl apply \
    -f kubernetes/average-shade.config-map.yml \
    -f kubernetes/average-shade.stateful.yml \
    -f kubernetes/average-shade.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `average_shade` documentation on <http://localhost:8282/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.


### digit_recognition


- [Code](../../services/digit_recognition)
- digit_recognition URL when run locally: <http://localhost:8383/docs>
- digit_recognition URL when deployed on Fribourg's Kubernetes: <https://digit-recognition-csia-pme.kube.isc.heia-fr.ch/docs>

## Description

This service uses a keras model to guess a digit in an image.

The service is built in two steps:

1. [Model creation](#model-creation) - The creation of the model from the data
2. [Model serving](#model-serving) - The serving of the built model

## Model creation

The goal of this step is to prepare the data and train a new model. All further commands are ran in the [model_creation](../../services/digit_recognition/model_creation) directory.

### Run the experiment

The model can be tweaked using the [`params.yaml`](../../services/digit_recognition/model_creation/params.yaml) file. The `numbers` parameter allows to indicate the digits the model must be able to detect.

Run a new training using the following commands.

```sh
# Export the MinIO S3 credentials (ask them to other members of the team)
export AWS_ACCESS_KEY_ID=***
export AWS_SECRET_ACCESS_KEY=***

# Pull the required data for the experiment from MinIO
dvc pull

# Reproduce the ML experiment with DVC
dvc repro
```

The DVC pipeline is described in the [`dvc.yaml`](../../services/digit_recognition/model_creation/dvc.yaml) file.

Each stage describes the dependencies and the outputs of the stage. Every time a dependency of the experiment is updated, running `dvc repro` will run the stages of the pipeline that are affected and keep the results in cache to speed up future runs.

More information on their website: [_Get Started: Data Pipelines_ - dvc.org](https://dvc.org/doc/start/data-management/data-pipelines).

### Push new data/results to MinIO

In order to push new results to MinIO, use the following commands (similar to Git). **Note**: DVC automatically adds files that are specified in the pipelines. In other words, there are no needs to explicitely add those files with `dvc add`.

```sh
# Get the data status
dvc status

# Add the required files to DVC
dvc add <the files you would add to DVC>

# Push the data to DVC
dvc push
```

## Model serving

The goal of this step is to serve the model made in the previous step. All further commands are ran in the [model_serving](../../services/digit_recognition/model_serving) directory.

The API documentation is automatically generated by FastAPI using the OpenAPI standard. A user friendly interface provided by Swagger is available under the `/docs` route, where the endpoints of teh service are described.

This simple service only has one route `/compute` that takes an image as input, which will be used to guess the number.

### Retrieve the model

Run the following command to get the model created from the previous step.

```sh
# Copy the model from the creation directory
cp ../model_creation/mnist_model.h5 .
```

### How to run

#### Environment variables

The service will use the following environment variables if defined.

*General variables*

- `APP_HOST`: address on which the API will listen, default is 127.0.0.1
- `APP_PORT`: port the API will listen on, default is 8080
- `APP_LOG`: log level, default is info
- `APP_ENGINE`: the url to the engine, if provided, the service will announce itself to the engine periodically
- `APP_SERVICE`: the url of the service itself, needed to announce correct routes to the engine
- `APP_NOTIFY_CRON`: the frequency in second of the heartbeat announce to the engine, default is 30

#### Start the application

In the [model_serving](../../services/digit_recognition/model_serving) directory, start the service with the following commands.

Generate the virtual environment and install the dependencies.

```sh
# Generate the virtual environment
python3 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate

# Install the requirements
pip install --requirement requirements.txt
```

Start the application.

```sh
# Start the application
APP_HOST=0.0.0.0 APP_PORT=8383 APP_ENGINE=http://localhost:8080 APP_SERVICE=http://localhost:8383 python3 main.py
```

Access the `digit_recognition` documentation on <http://localhost:8383/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.

#### Run the tests

In the [model_serving](../../services/digit_recognition/model_serving) directory, run the tests with the following commands.

Install the additional packages.

```sh
# Install required packages for testing
pip3 install pytest pytest-asyncio aiofile
```

Run the tests.

```sh
# Run the tests
python3 -m pytest --asyncio-mode=auto
```

#### Run locally using Kubernetes (with minikube) and official Docker images

Start the service with the following commands. This will start the service with the official Docker images that are hosted on GitHub.

In the [model_serving](../../services/digit_recognition/model_serving) directory, start the service with the following commands.

```sh
# Start the digit_recognition backend
kubectl apply \
    -f kubernetes/digit-recognition.config-map.yml \
    -f kubernetes/digit-recognition.stateful.yml \
    -f kubernetes/digit-recognition.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `digit_recognition` documentation on <http://localhost:8383/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.

#### Run locally using Kubernetes (with minikube) and a local Docker image

**Note**: The service StatefulSet (`digit-recognition.stateful.yml` file) must be deleted and recreated every time a new Docker image is created.

Start the service with the following commands. This will start the service with the a local Docker image for the service.

In the [model_serving](../../services/digit_recognition/model_serving) directory, build the Docker image with the following commands.

```sh
# Access the Minikube's Docker environment
eval $(minikube docker-env)

# Build the Docker image
docker build -t ghcr.io/csia-pme/csia-pme-digit-recognition:latest .

# Exit the Minikube's Docker environment
eval $(minikube docker-env -u)

# Edit the `kubernetes/digit-recognition.stateful.yml` file to use the local image by uncommented the line `imagePullPolicy`
#
# From
#
#        # imagePullPolicy: Never
#
# To
#
#        imagePullPolicy: Never
```

In the [model_serving](../../services/digit_recognition/model_serving) directory, start the service with the following commands.

```sh
# Start the digit_recognition backend
kubectl apply \
    -f kubernetes/digit-recognition.config-map.yml \
    -f kubernetes/digit-recognition.stateful.yml \
    -f kubernetes/digit-recognition.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `digit_recognition` documentation on <http://localhost:8383/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.





### face_analyzer


- [Code](../../services/face_analyzer)
- face_analyzer URL when run locally: <http://localhost:8484/docs>
- face_analyzer URL when deployed on Fribourg's Kubernetes: <https://face-analyzer-csia-pme.kube.isc.heia-fr.ch/docs>

## Description

This service uses machine learning models to process a face in an image in order to guess the gender, the age, the race and the emotion.

The API documentation is automatically generated by FastAPI using the OpenAPI standard. A user friendly interface provided by Swagger is available under the `/docs` route, where the endpoints of teh service are described.

This simple service only has one route `/compute` that takes an image as input, which will be used to process the face, if any. Warning, if the image contains several people, only one will be processed.

## How to run

### Environment variables

The service will use the following environment variables if defined.

*General variables*

- `APP_HOST`: address on which the API will listen, default is 127.0.0.1
- `APP_PORT`: port the API will listen on, default is 8080
- `APP_LOG`: log level, default is info
- `APP_ENGINE`: the url to the engine, if provided, the service will announce itself to the engine periodically
- `APP_SERVICE`: the url of the service itself, needed to announce correct routes to the engine
- `APP_NOTIFY_CRON`: the frequency in second of the heartbeat announce to the engine, default is 30

### Start the application

In the [face_analyzer](../../services/face_analyzer) directory, start the service with the following commands.

Generate the virtual environment and install the dependencies.

```sh
# Generate the virtual environment
python3 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate

# Install the requirements
pip install --requirement requirements.txt
```

Start the application.

```sh
# Start the application
APP_HOST=0.0.0.0 APP_PORT=8484 APP_ENGINE=http://localhost:8080 APP_SERVICE=http://localhost:8484 python3 main.py
```

Access the `face_analyzer` documentation on <http://localhost:8484/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.

### Run the tests

In the [face_analyzer](../../services/face_analyzer) directory, run the tests with the following commands.

Install the additional packages.

```sh
# Install required packages for testing
pip3 install pytest pytest-asyncio aiofile
```

Run the tests.

```sh
# Run the tests
python3 -m pytest --asyncio-mode=auto
```

### Run locally using Kubernetes (with minikube) and official Docker images

Start the service with the following commands. This will start the service with the official Docker images that are hosted on GitHub.

In the [face_analyzer](../../services/face_analyzer) directory, start the service with the following commands.

```sh
# Start the face_analyzer backend
kubectl apply \
    -f kubernetes/face-analyzer.config-map.yml \
    -f kubernetes/face-analyzer.stateful.yml \
    -f kubernetes/face-analyzer.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `face_analyzer` documentation on <http://localhost:8484/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.

### Run locally using Kubernetes (with minikube) and a local Docker image

**Note**: The service StatefulSet (`face-analyzer.stateful.yml` file) must be deleted and recreated every time a new Docker image is created.

Start the service with the following commands. This will start the service with the a local Docker image for the service.

In the [face_analyzer](../../services/face_analyzer) directory, build the Docker image with the following commands.

```sh
# Access the Minikube's Docker environment
eval $(minikube docker-env)

# Build the Docker image
docker build -t ghcr.io/csia-pme/csia-pme-face-analyzer:latest .

# Exit the Minikube's Docker environment
eval $(minikube docker-env -u)

# Edit the `kubernetes/face-analyzer.stateful.yml` file to use the local image by uncommented the line `imagePullPolicy`
#
# From
#
#        # imagePullPolicy: Never
#
# To
#
#        imagePullPolicy: Never
```

In the [face_analyzer](../../services/face_analyzer) directory, start the service with the following commands.

```sh
# Start the face_analyzer backend
kubectl apply \
    -f kubernetes/face-analyzer.config-map.yml \
    -f kubernetes/face-analyzer.stateful.yml \
    -f kubernetes/face-analyzer.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `face_analyzer` documentation on <http://localhost:8484/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.


### face_detection


- [Code](../../services/face_detection)
- face_detection URL when run locally: <http://localhost:8585/docs>
- face_detection URL when deployed on Fribourg's Kubernetes: <https://face-detection-csia-pme.kube.isc.heia-fr.ch/docs>

## Description

This service uses a machine learning model to detect all human faces in an image. The result of this process is a simple list of bounding boxes in px.

The API documentation is automatically generated by FastAPI using the OpenAPI standard. A user friendly interface provided by Swagger is available under the `/docs` route, where the endpoints of teh service are described.

This simple service only has one route `/compute` that takes an image as input, which will be used to detect the faces, if any.

## How to run

### Environment variables

The service will use the following environment variables if defined.

*General variables*

- `APP_HOST`: address on which the API will listen, default is 127.0.0.1
- `APP_PORT`: port the API will listen on, default is 8080
- `APP_LOG`: log level, default is info
- `APP_ENGINE`: the url to the engine, if provided, the service will announce itself to the engine periodically
- `APP_SERVICE`: the url of the service itself, needed to announce correct routes to the engine
- `APP_NOTIFY_CRON`: the frequency in second of the heartbeat announce to the engine, default is 30

### Start the application

In the [face_detection](../../services/face_detection) directory, start the service with the following commands.

Generate the virtual environment and install the dependencies.

```sh
# Generate the virtual environment
python3 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate

# Install the requirements
pip install --requirement requirements.txt
```

Start the application.

```sh
# Start the application
APP_HOST=0.0.0.0 APP_PORT=8585 APP_ENGINE=http://localhost:8080 APP_SERVICE=http://localhost:8585 python3 main.py
```

Access the `face_detection` documentation on <http://localhost:8585/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.

### Run the tests

In the [face_detection](../../services/face_detection) directory, run the tests with the following commands.

Install the additional packages.

```sh
# Install required packages for testing
pip3 install pytest pytest-asyncio aiofile
```

Run the tests.

```sh
# Run the tests
python3 -m pytest --asyncio-mode=auto
```

### Run locally using Kubernetes (with minikube) and official Docker images

Start the service with the following commands. This will start the service with the official Docker images that are hosted on GitHub.

In the [face_detection](../../services/face_detection) directory, start the service with the following commands.

```sh
# Start the face_detection backend
kubectl apply \
    -f kubernetes/face-detection.config-map.yml \
    -f kubernetes/face-detection.stateful.yml \
    -f kubernetes/face-detection.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `face_detection` documentation on <http://localhost:8585/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.

### Run locally using Kubernetes (with minikube) and a local Docker image

**Note**: The service StatefulSet (`face-detection.stateful.yml` file) must be deleted and recreated every time a new Docker image is created.

Start the service with the following commands. This will start the service with the a local Docker image for the service.

In the [face_detection](../../services/face_detection) directory, build the Docker image with the following commands.

```sh
# Access the Minikube's Docker environment
eval $(minikube docker-env)

# Build the Docker image
docker build -t ghcr.io/csia-pme/csia-pme-face-detection:latest .

# Exit the Minikube's Docker environment
eval $(minikube docker-env -u)

# Edit the `kubernetes/face-detection.stateful.yml` file to use the local image by uncommented the line `imagePullPolicy`
#
# From
#
#        # imagePullPolicy: Never
#
# To
#
#        imagePullPolicy: Never
```

In the [face_detection](../../services/face_detection) directory, start the service with the following commands.

```sh
# Start the face_detection backend
kubectl apply \
    -f kubernetes/face-detection.config-map.yml \
    -f kubernetes/face-detection.stateful.yml \
    -f kubernetes/face-detection.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `face_detection` documentation on <http://localhost:8585/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.


### image_processing


- [Code](../../services/image_processing/README.md)
- average_shape URL when run locally: <http://localhost:8181/docs>
- average_shape URL when deployed on Fribourg's Kubernetes: <https://image-processing-csia-pme.kube.isc.heia-fr.ch/docs>

## Description

This service provides generic image processing features, such as blurring, cropping, resizing etc... This service uses Pillow and opencv.

## How to run

### Environment variables

The service will use the following environment variables if defined.

*General variables*

- `APP_HOST`: address on which the API will listen, default is 127.0.0.1
- `APP_PORT`: port the API will listen on, default is 8080
- `APP_LOG`: log level, default is info
- `APP_ENGINE`: the url to the engine, if provided, the service will announce itself to the engine periodically
- `APP_SERVICE`: the url of the service itself, needed to announce correct routes to the engine
- `APP_NOTIFY_CRON`: the frequency in second of the heartbeat announce to the engine, default is 30

### Start the application

In the [image_processing](../../services/image_processing) directory, start the service with the following commands.

Generate the virtual environment and install the dependencies.

```sh
# Generate the virtual environment
python3 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate

# Install the requirements
pip install --requirement requirements.txt
```

Start the application.

```sh
# Start the application
APP_HOST=0.0.0.0 APP_PORT=8181 APP_ENGINE=http://localhost:8080 APP_SERVICE=http://localhost:8181 python3 main.py
```

Access the `image_processing` documentation on <http://localhost:8181/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.

### Run the tests

In the [image_processing](../../services/image_processing) directory, run the tests with the following commands.

Install the additional packages.

```sh
# Install required packages for testing
pip3 install pytest pytest-asyncio aiofile
```

Run the tests.

```sh
# Run the tests
python3 -m pytest --asyncio-mode=auto
```

### Run locally using Kubernetes (with minikube) and official Docker images

Start the service with the following commands. This will start the service with the official Docker images that are hosted on GitHub.

In the [image_processing](../../services/image_processing) directory, start the service with the following commands.

```sh
# Start the image_processing backend
kubectl apply \
    -f kubernetes/image-processing.config-map.yml \
    -f kubernetes/image-processing.stateful.yml \
    -f kubernetes/image-processing.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `image_processing` documentation on <http://localhost:8181/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.

### Run locally using Kubernetes (with minikube) and a local Docker image

**Note**: The service StatefulSet (`image-processing.stateful.yml` file) must be deleted and recreated every time a new Docker image is created.

Start the service with the following commands. This will start the service with the a local Docker image for the service.

In the [image_processing](../../services/image_processing) directory, build the Docker image with the following commands.

```sh
# Access the Minikube's Docker environment
eval $(minikube docker-env)

# Build the Docker image
docker build -t ghcr.io/csia-pme/csia-pme-image-processing:latest .

# Exit the Minikube's Docker environment
eval $(minikube docker-env -u)

# Edit the `kubernetes/image-processing.stateful.yml` file to use the local image by uncommented the line `imagePullPolicy`
#
# From
#
#        # imagePullPolicy: Never
#
# To
#
#        imagePullPolicy: Never
```

In the [image_processing](../../services/image_processing) directory, start the service with the following commands.

```sh
# Start the image_processing backend
kubectl apply \
    -f kubernetes/image-processing.config-map.yml \
    -f kubernetes/image-processing.stateful.yml \
    -f kubernetes/image-processing.service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `image_processing` documentation on <http://localhost:8181/docs>.

Access the Engine documentation on <http://localhost:8080/docs> to validate the backend has been successfully registered to the Engine.

